{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f24d3a1-0334-44f5-a3fb-67d3a0d7995c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement cv2 (from versions: none)\n",
      "ERROR: No matching distribution found for cv2\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\Omen\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\omen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\omen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\omen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\omen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\omen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\omen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\omen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (70.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\omen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\Omen\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install cv2\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install matplotlib\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "673bc581-cdb3-47f4-9122-f4c4e2ef2250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EmotionRecognitionCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EmotionRecognitionCNN, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # Первый блок сверток\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # 3 канала для цветных изображений\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Уменьшение размерности\n",
    "\n",
    "            # Второй блок сверток\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Уменьшение размерности\n",
    "\n",
    "            # Третий блок сверток\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Уменьшение размерности\n",
    "        )\n",
    "\n",
    "        # Классификатор\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 6 * 6, 1024),  # Входной размер: 256 каналов после сверток\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes)  # Выходной слой с числом классов\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # Применение сверток\n",
    "        x = x.view(x.size(0), -1)  # Разворачиваем тензор для подачи в fully connected слои\n",
    "        x = self.classifier(x)  # Применение классификатора\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c80a0c-a530-4d74-bde2-a9d414e3e594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Omen\\AppData\\Local\\Temp\\ipykernel_18336\\893831113.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('emotion_recognition_model_V4.pth'))\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "# Загрузка обученной модели\n",
    "model = EmotionRecognitionCNN(num_classes=6)\n",
    "model.load_state_dict(torch.load('emotion_recognition_model_V4.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Загрузка классификатора Хаара для обнаружения лиц\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Преобразования для предобработки изображения\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Используем три канала\n",
    "])\n",
    "\n",
    "# Метки эмоций\n",
    "emotions = ['Angry', 'Fearful', 'Happy', 'Sad', 'Surprised', 'Neutral']\n",
    "\n",
    "# Запуск камеры\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Захват изображения с камеры\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Обнаружение лиц на изображении\n",
    "    faces = face_cascade.detectMultiScale(frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Извлечение лица\n",
    "        face = frame[y:y + h, x:x + w]\n",
    "\n",
    "        # Преобразование для модели\n",
    "        face_img = transform(face).unsqueeze(0)  # Добавляем размер батча\n",
    "\n",
    "        # Предсказание эмоции\n",
    "        with torch.no_grad():\n",
    "            output = model(face_img)\n",
    "            emotion_prediction = F.softmax(output, dim=1)\n",
    "            emotion_label = torch.argmax(emotion_prediction).item()\n",
    "\n",
    "        # Метка предсказанной эмоции\n",
    "        emotion_text = emotions[emotion_label]\n",
    "\n",
    "        # Отображение предсказанной эмоции на изображении\n",
    "        cv2.putText(frame, emotion_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "    # Показ изображения\n",
    "    cv2.imshow('Emotion Recognition', frame)\n",
    "\n",
    "    # Прерывание по нажатию клавиши 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Освобождение камеры и закрытие окон\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040bef35-a1de-46cf-a6ea-7f90d9be85ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
